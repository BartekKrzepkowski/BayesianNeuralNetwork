{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian neural network is a neural network with a prior distribution on its weights.<br>\n",
    "With contrast to well known neural networks, where weights are point estimated, weights in bayesian neural network are interval estimated, with a given confidence interval.<br>\n",
    "This approach allows for a more thorough examination of the learned model, along with the analysis of uncertainty as to the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python libraries associated with Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edward\n",
    "http://edwardlib.org/<br>\n",
    "\n",
    "\n",
    "#### InferPy\n",
    "https://inferpy.readthedocs.io/projects/develop/en/latest/notes/guidebayesian.html<br>\n",
    "\n",
    "\n",
    "#### PyPyro\n",
    "http://pyro.ai/numpyro/index.html<br>\n",
    "\n",
    "\n",
    "#### PyMC3\n",
    "https://docs.pymc.io/<br>\n",
    "\n",
    "\n",
    "#### PyStan\n",
    "https://pystan.readthedocs.io/en/latest/index.html<br>\n",
    "\n",
    "\n",
    "#### TensorFlow Probability\n",
    "https://www.tensorflow.org/probability/overview<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box's loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model_infer_criticize.png\" width=1000 height=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) Build a probabilistic model of the phenomena.<br>\n",
    "> 2) Reason about the phenomena given model and data.<br>\n",
    "> 3) Criticize the model, revise and repeat<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main notions\n",
    "\n",
    "Given observable phenomena we can consider, \"What distribution is sufficient to reproduce their observed occurrence?\".<br>\n",
    "Let's this source distribution be coded within random variable $X$.<br>\n",
    "The distribution of the $X$ variable can be really complicated, which is why we try to approximate it, with a distribution well studied in the literature.<br>\n",
    "We can look at this as postulating a variable $Z$, with an unknown distribution, which is in some correlation with the observable data that is encoded with the variable X.<br>\n",
    "Therefore, we want $Z$ to have a descriptive distribution that explains the data well.<br>\n",
    "We can start our considerations, by examining the joint distribution $P(X,Z)$, which, in the rest of the text, will be called as probabilistic model.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### The prior\n",
    "$$P(Z)$$\n",
    "Distribution of postulated latent variable.It is our hypotesis about latent variable.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### The posterior\n",
    "$$P(Z|X)$$\n",
    "Distribution of postulated latent variable, given some phenomena.<br>\n",
    "By accepting Bayes' law, it is possible to calculate this probability in this way:<br>\n",
    "$$P(Z|X) = \\frac{P(X, Z)}{P(X)} = \\frac{P(X, Z)}{\\int P(X, Z^{'})dZ^{'}}$$\n",
    "From the perspective of inductivism: The posterior is our updated hypothesis, about the latent variables.<br>\n",
    "From the perspective of deductivism: The posterior is model, which is fitted to observable data.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### The likelihood\n",
    "$$P(X|Z)$$\n",
    "Distribution of observable phenomena, given latent variable.<br>\n",
    "Tak jak wyżej, akceptując prawo Bayesa, możemy obliczyć to wyrażenie, w następujący sposób.\n",
    "$$P(X|Z) = \\frac{P(Z|X) \\dot P(X)}{P(Z)}$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Conjugate distributions\n",
    "If distribution of $\\textbf{prior}$ and $\\textbf{posterior}$ are in the same probaility distribution family, then they are called $\\textbf{conjugate distributions}$, and the $\\textbf{prior}$ is called a $\\textbf{conjugate prior}$ for the likelihood function.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Bayes estimator\n",
    "Given measurements x of some phenomena.<br>\n",
    "If estimated parameter $\\theta = \\theta(x)$ has a prior distribution $\\pi$, ${\\widehat {\\theta }}={\\widehat {\\theta }}(x)$ is an estimator of $\\theta$ and $L(\\theta ,{\\widehat {\\theta }})$ is a loss function, proper to given measurements, then estimator ${\\widehat {\\theta }}$ which minimizes $E_{\\pi }(L(\\theta ,{\\widehat {\\theta }}))$, is said to be a $\\textbf{Bayes estimator}$. Mentioned expection is called a $\\textbf{Bayes risk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of Probabilistic Models\n",
    "\n",
    "### Inferring the posterior\n",
    "The posterior is difficult to compute because of its normalizing constant, which is the integral in the denominator. This is often a high-dimensional integral that lacks an analytic (closed-form) solution. Thus, calculating the posterior means approximating the posterior.<br>\n",
    "In inference-related tasks, inference adjusts parameters of the distribution, coded within latent variable, to be close to the posterior.\n",
    "\n",
    "\n",
    "## Classes of Inference\n",
    "\n",
    "### Variational Interference\n",
    "Bunch of algorithms which cast inference as optimatization problem.<br>\n",
    "The distribution of posterior is chosen with regards to KL-divergence, which measures distance between the estimated distribution and posit a family of approximating distributions family assumed in advance <br>\n",
    "Edward takes the perspective that the posterior is (typically) intractable, and thus we must build a model of latent variables that best approximates the posterior.\n",
    "\n",
    "### Monte Carlo\n",
    "This approach approximates the posterior using samples.We choose approximation of the posterior using constructed empirical cumulative distributors.\n",
    "\n",
    "\n",
    "### Exact Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
